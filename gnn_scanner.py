import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data
import pandas as pd
from sklearn.preprocessing import LabelEncoder


class MyGNN(nn.Module):
    def __init__(self, num_features, hidden_channels, num_classes):
        super(MyGNN, self).__init__()

        # Capa de entrada
        self.conv1 = GCNConv(num_features, hidden_channels)

        # Capa oculta
        self.conv2 = GCNConv(hidden_channels, hidden_channels)

        # Capa de salida
        self.conv3 = GCNConv(hidden_channels, num_classes)

    def forward(self, data):
        edge_index, edge_attr, batch = data.edge_index, data.edge_attr, data.batch

        # Aplicar capas GCN y funciones de activación
        x = F.relu(self.conv1(edge_attr, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = self.conv3(x, edge_index)


        # Global pooling para obtener la salida del grafo

        x = global_mean_pool(x, batch)

        # Aplicar la función de activación adecuada para tu tarea
        x = F.log_softmax(x, dim=1)  # Mantén F.log_softmax si es un problema de clasificación
        # Ajustar la forma de la salida
        x = x.unsqueeze(0)  # Añadir una dimensión extra
        x = x.view(-1, num_classes)

        return x
# Definir los parámetros del modelo
num_features = 7  # Reemplaza con el número real de características
hidden_channels = 64  # Puedes ajustar este valor según sea necesario
num_classes = 2  # Reemplaza con el número real de clases

# Crear una instancia del modelo
model = MyGNN(num_features, hidden_channels, num_classes)
model
# Later, when you want to perform inference
# Load the entire model
loaded_model = torch.load('gnn/whole_graph_model.pth')
loaded_model.eval()

# Cargar el archivo CSV en un DataFrame
network_packets_df = pd.read_csv('network_packets.csv')

# Guardar copias de las direcciones IP originales
network_packets_df['original_srcip'] = network_packets_df['srcip']
network_packets_df['original_dstip'] = network_packets_df['dstip']

# Eliminar valores nulos y reemplazar espacios
network_packets_df = network_packets_df.fillna('Normal')
network_packets_df = network_packets_df.replace(' ', 'Normal')

# Columnas que quieres convertir
columns_to_encode = ['state', 'proto', 'srcip', 'dstip']

# Aplicar Label Encoding
label_encoder_train = LabelEncoder()
for column in columns_to_encode:
    network_packets_df[column] = label_encoder_train.fit_transform(network_packets_df[column])

# Función para convertir valores a cero
def convertir_a_cero(valor):
    return 0 if isinstance(valor, str) else valor

# Aplicar la función a las columnas deseada
network_packets_df['sport'] = network_packets_df['sport'].apply(convertir_a_cero)
network_packets_df['dsport'] = network_packets_df['dsport'].apply(convertir_a_cero)
network_packets_df['sport'] = network_packets_df['sport'].astype('int64')
network_packets_df['dsport'] = network_packets_df['dsport'].astype('int64')

# Procesar los nuevos datos
edge_index_list = []
edge_attr_features = ['proto', 'state', 'dur', 'sbytes', 'dbytes', 'sport', 'dsport']
edge_attr_values = network_packets_df[edge_attr_features].to_numpy()

for i in range(len(network_packets_df)):
    edge_index_i = torch.tensor([network_packets_df.iloc[i]['srcip'], network_packets_df.iloc[i]['dstip']], dtype=torch.long)
    edge_index_list.append(edge_index_i)

edge_attr_list = [torch.tensor(edge_attr_i, dtype=torch.float) for edge_attr_i in edge_attr_values]

# Assuming you don't have labels for new data (as it's inference)
y = None

# Create a list of Data objects
new_data_processed = [Data(x=edge_index_list[i].view(1, -1),  
                           edge_index=edge_index_list[i].view(-1, 1),  
                           edge_attr=edge_attr_list[i].view(1, -1),
                           y=y) for i in range(len(network_packets_df))]

# Realizar inferencia en los nuevos datos
with torch.no_grad():
    all_predictions = []
    for data in new_data_processed:
        data.edge_index = data.edge_index.view(-1)
        data.edge_index[0]= 0
        data.edge_index[1]= 0
        output = loaded_model(data)
        probabilities = torch.sigmoid(output)
        predictions = torch.argmax(probabilities, dim=1).int().item()
        all_predictions.append(predictions)


# Añadir las predicciones al DataFrame
network_packets_df['predictions'] = all_predictions

# Filtrar las filas donde la predicción es 1
predicted_ones_df = network_packets_df[network_packets_df['predictions'] == 1]

# Eliminar duplicados basados en las columnas 'original_srcip' y 'original_dstip'
# Esto asegura que solo se mantenga una fila para cada combinación única de 'original_srcip' y 'original_dstip'
predicted_ones_df_unique = predicted_ones_df.drop_duplicates(subset=['original_srcip', 'original_dstip'])

# Conservar solo las columnas relevantes, incluyendo las IPs originales
columns_to_keep = ['original_srcip', 'sport', 'original_dstip', 'dsport', 'proto', 'state', 'dur', 
                   'sbytes', 'dbytes', 'Spkts', 'Dpkts', 'swin', 'dwin', 'tcprtt',
                   'synack', 'ackdat', 'ct_flw_http_mthd', 'predictions']
predicted_ones_df_unique = predicted_ones_df_unique[columns_to_keep]

# Guardar las filas filtradas en un nuevo archivo CSV
predicted_ones_df_unique.to_csv('predicted_ones_output.csv', index=False)
