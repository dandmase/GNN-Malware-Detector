import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import warnings
import os

class MyGNN(nn.Module):
    def __init__(self, num_features, hidden_channels, num_classes):
        super(MyGNN, self).__init__()
        # Input layer
        self.conv1 = GCNConv(num_features, hidden_channels)
        # Hidden layer
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        # Output layer
        self.conv3 = GCNConv(hidden_channels, num_classes)

    def forward(self, data):
        edge_index, edge_attr, batch = data.edge_index, data.edge_attr, data.batch
        # Apply GCN layers and activation functions
        x = F.relu(self.conv1(edge_attr, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = self.conv3(x, edge_index)
        # Global pooling to obtain graph output
        x = global_mean_pool(x, batch)
        # Apply the appropriate activation function for your task
        x = F.log_softmax(x, dim=1)  # Keep F.log_softmax if it's a classification problem
        # Reshape the output
        x = x.unsqueeze(0)  # Add an extra dimension
        x = x.view(-1, num_classes)
        return x

# Define model parameters
num_features = 7  # Replace with the actual number of features
hidden_channels = 64  # You can adjust this value as needed
num_classes = 2  # Replace with the actual number of classes

# Create an instance of the model
model = MyGNN(num_features, hidden_channels, num_classes)
model

# Later, when you want to perform inference
# Load the entire model
loaded_model = torch.load('gnn/whole_graph_model.pth')
loaded_model.eval()

# Load the CSV file into a DataFrame
network_packets_df = pd.read_csv('network_packets.csv')

# Save copies of the original IP addresses
network_packets_df['original_srcip'] = network_packets_df['srcip']
network_packets_df['original_dstip'] = network_packets_df['dstip']

# Remove null values and replace spaces
network_packets_df = network_packets_df.fillna('Normal')
network_packets_df = network_packets_df.replace(' ', 'Normal')

# Columns you want to encode
columns_to_encode = ['state', 'proto', 'srcip', 'dstip']

# Apply Label Encoding
label_encoder_train = LabelEncoder()
for column in columns_to_encode:
    network_packets_df[column] = label_encoder_train.fit_transform(network_packets_df[column])

# Function to convert values to zero
def convert_to_zero(value):
    return 0 if isinstance(value, str) else value

# Apply the function to the desired columns
network_packets_df['sport'] = network_packets_df['sport'].apply(convert_to_zero)
network_packets_df['dsport'] = network_packets_df['dsport'].apply(convert_to_zero)
network_packets_df['sport'] = network_packets_df['sport'].astype('int64')
network_packets_df['dsport'] = network_packets_df['dsport'].astype('int64')

# Process the new data
edge_index_list = []
edge_attr_features = ['proto', 'state', 'dur', 'sbytes', 'dbytes', 'sport', 'dsport']
edge_attr_values = network_packets_df[edge_attr_features].to_numpy()

for i in range(len(network_packets_df)):
    edge_index_i = torch.tensor([network_packets_df.iloc[i]['srcip'], network_packets_df.iloc[i]['dstip']], dtype=torch.long)
    edge_index_list.append(edge_index_i)

edge_attr_list = [torch.tensor(edge_attr_i, dtype=torch.float) for edge_attr_i in edge_attr_values]

# Assuming you don't have labels for new data (as it's inference)
y = None

# Create a list of Data objects
new_data_processed = [Data(x=edge_index_list[i].view(1, -1),  
                           edge_index=edge_index_list[i].view(-1, 1),  
                           edge_attr=edge_attr_list[i].view(1, -1),
                           y=y) for i in range(len(network_packets_df))]

# Define the columns to keep
columns_to_keep = ['original_srcip', 'sport', 'original_dstip', 'dsport', 'proto', 'state', 'dur', 
                   'sbytes', 'dbytes', 'Spkts', 'Dpkts', 'swin', 'dwin', 'tcprtt',
                   'synack', 'ackdat', 'ct_flw_http_mthd', 'predictions']

# Create an empty DataFrame with the desired columns
predicted_ones_df = pd.DataFrame(columns=columns_to_keep)

# Check if the 'predicted_ones_output.csv' file exists before adding headers
if not os.path.exists('predicted_ones_output.csv'):
    predicted_ones_df.to_csv('predicted_ones_output.csv', index=False, mode='w')

indices_to_drop = []
# Loop to process rows individually and write row by row to the CSV
for index, row in network_packets_df.iterrows():
    # Prepare input for inference
    data = new_data_processed[index]
    with torch.no_grad():
        data.edge_index = data.edge_index.view(-1)
        data.edge_index[0] = 0
        data.edge_index[1] = 0
        output = loaded_model(data)
        probabilities = torch.sigmoid(output)
        predictions = torch.argmax(probabilities, dim=1).int().item()

    # Add predictions to the row
    row['predictions'] = predictions

    # Filter the row if the prediction is 1
    if row['predictions'] == 1:
        # Check if the processed row already exists in 'predicted_ones_df'
        is_duplicate = (
            (predicted_ones_df['original_srcip'] == row['original_srcip']) &
            (predicted_ones_df['original_dstip'] == row['original_dstip'])
        ).any()

        # If it's not a duplicate, continue
        if not is_duplicate:
            # Keep only the relevant columns
            relevant_row = row[columns_to_keep]

            # Convert 'relevant_row' to a DataFrame if it's not already
            if not isinstance(relevant_row, pd.DataFrame):
                relevant_row = pd.DataFrame([relevant_row])

            # Ensure that 'relevant_row' has the same set of columns as 'predicted_ones_df'
            relevant_row = relevant_row.reindex(columns=predicted_ones_df.columns)

            with warnings.catch_warnings():
                warnings.simplefilter(action='ignore', category=FutureWarning)
                # Add 'relevant_row' to 'predicted_ones_df'
                predicted_ones_df = pd.concat([predicted_ones_df, relevant_row], ignore_index=True)

                # Write the row to the CSV file (mode 'a' to append)
                relevant_row.to_csv('predicted_ones_output.csv', index=False, mode='a', header=False)

    # Add index to the list of indices to drop
    indices_to_drop.append(index)

# Remove the processed rows from 'network_packets_df'
network_packets_df = network_packets_df.drop(indices_to_drop)

# Save the updated DataFrame (with processed rows removed) to the original CSV file
network_packets_df.to_csv('network_packets.csv', index=False)
